{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fb0362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f91dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_parquet('features.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39b7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_clmns = ['property_location',\n",
    "'parcel_number',\n",
    "'lot',\n",
    "'current_sales_date',\n",
    "'assessed_fixtures_value',\n",
    "'assessed_improvement_value',\n",
    "'assessed_land_value',\n",
    "'assessed_personal_property_value',\n",
    "'the_geom',\n",
    "'sqft_price',\n",
    "'home_price']\n",
    "\n",
    "#     'property_location': 'object',  # part of UID\n",
    "#     'parcel_number': 'object',  # part of UID\n",
    "#     'block': 'object',  # part of UID\n",
    "#     'lot': 'object',  # part of UID\n",
    "\n",
    "features['idx'] = (\n",
    "    features.property_location +\n",
    "    features.parcel_number +\n",
    "    features.block  +\n",
    "    features.lot +\n",
    "    features.week_number.astype('str')\n",
    ")\n",
    "features.set_index('idx', inplace=True)\n",
    "main_feats = pd.read_csv('main_feats.csv').cols.iloc[:50]\n",
    "data = features.drop(columns=drop_clmns).loc[:, main_feats]\n",
    "\n",
    "target = features['home_price'] / features.home_price_lag1_roll26_rolling_median\n",
    "target_sq = features['sqft_price'] / features.sqft_price_lag1_roll26_rolling_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11fb4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask = (target_sq > target_sq.quantile(0.05)) & ( target_sq < target_sq.quantile(0.95))\n",
    "week_mask = data.week_number > 51\n",
    "\n",
    "data = data.loc[target_mask & week_mask]\n",
    "target_sq = target_sq.loc[target_mask & week_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2375a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [name for name, type_ in data.dtypes.iteritems() if type_ == 'object']\n",
    "for cat in cats:\n",
    "    data[cat] = data[cat].astype('category')\n",
    "\n",
    "cats = [name for name, type_ in data.dtypes.iteritems() if type_ == 'category']\n",
    "for cat in cats:\n",
    "    data[cat] = data[cat].cat.codes\n",
    "data = data.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4bf24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "years, week_step = 5, 4\n",
    "\n",
    "start = 51\n",
    "end = start + 52*years\n",
    "number_of_steps = (data.week_number.max() - end)//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a95acf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_number = data.week_number\n",
    "data = data.drop(columns=['week_number','block'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99728d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGBOOST:\n",
    "    \"\"\"\n",
    "    NGBoost regressor predicts mean and standard deviation for given Xi.\n",
    "    Any quantile could be calculated using these,\n",
    "    or standard deviation could be used as a feature directly.\n",
    "    https://github.com/stanfordmlgroup/ngboost/blob/master/ngboost/ngboost.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.best_params = {'ngb_n_estimators': 40, 'base_max_depth': 14, 'base_min_samples_leaf': 16}\n",
    "        \n",
    "        base_learner = DecisionTreeRegressor(\n",
    "                criterion='friedman_mse',\n",
    "                max_depth=self.best_params[\"base_max_depth\"],\n",
    "                min_samples_leaf=self.best_params[\"base_min_samples_leaf\"]\n",
    "                )\n",
    "        self.model = NGBRegressor(\n",
    "                verbose=False,\n",
    "                n_estimators=self.best_params[\"ngb_n_estimators\"],\n",
    "                Base=base_learner\n",
    "                )\n",
    "    \n",
    "        print(\"NGBoostRegressor initialized.\")\n",
    "\n",
    "    def finetune(self, X, y):\n",
    "        \"\"\"Note! For NGBoost you can tune both NGBoost \n",
    "        parameters and Base regressor parameters:\n",
    "        \n",
    "        NGBoost Parameters:\n",
    "            Dist              : assumed distributional form of Y|X=x.\n",
    "                                A distribution from ngboost.distns, e.g. Normal\n",
    "            Score             : rule to compare probabilistic predictions PÌ‚ to the observed data y.\n",
    "                                A score from ngboost.scores, e.g. LogScore\n",
    "            Base              : base learner to use in the boosting algorithm.\n",
    "                                Any instantiated sklearn regressor, e.g. DecisionTreeRegressor()\n",
    "            natural_gradient  : logical flag indicating whether the natural gradient should be used\n",
    "            n_estimators      : the number of boosting iterations to fit\n",
    "            learning_rate     : the learning rate\n",
    "            minibatch_frac    : the percent subsample of rows to use in each boosting iteration\n",
    "            verbose           : flag indicating whether output should be printed during fitting\n",
    "            verbose_eval      : increment (in boosting iterations) at which output should be printed\n",
    "            tol               : numerical tolerance to be used in optimization\n",
    "            random_state      : seed for reproducibility.\n",
    "                                See https://stackoverflow.com/questions/28064634/random-state-pseudo-random-number-in-scikit-learn\n",
    "            validation_fraction: Proportion of training data to set aside as validation data for early stopping.\n",
    "            early_stopping_rounds: The number of consecutive boosting iterations during which the\n",
    "                                        loss has to increase before the algorithm stops early.\n",
    "                                        Set to None to disable early stopping and validation.\n",
    "                                        None enables running over the full data set.\n",
    "\n",
    "        Base learner: DecisionTreeRegressor:\n",
    "            {'Base': DecisionTreeRegressor(criterion='friedman_mse', max_depth=4, max_features=None,\n",
    "                max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                min_impurity_split=None, min_samples_leaf=1,\n",
    "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                presort=False, random_state=None, splitter='best'), 'minibatch_frac': 1.0}\n",
    "        \"\"\"\n",
    "\n",
    "        def objective(trial):\n",
    "            \"\"\"\n",
    "            Optuna methods for suggestions:\n",
    "            suggest_categorical(name, choices)\n",
    "                Suggest a value for the categorical parameter.\n",
    "            suggest_discrete_uniform(name, low, high, q)\n",
    "                Suggest a value for the discrete parameter.\n",
    "            suggest_float(name, low, high, *[, step, log])\n",
    "                Suggest a value for the floating point parameter.\n",
    "            suggest_int(name, low, high[, step, log])\n",
    "                Suggest a value for the integer parameter.\n",
    "            suggest_loguniform(name, low, high)\n",
    "                Suggest a value for the continuous parameter.\n",
    "            suggest_uniform(name, low, high)\n",
    "                Suggest a value for the continuous parameter.\n",
    "            \"\"\"\n",
    "            # Use 'ngb_' prefix for NGBoost params:\n",
    "            ngb_n_estimators = trial.suggest_int(\"ngb_n_estimators\", 2, 200)\n",
    "            # {'ngb_n_estimators': 40, 'base_max_depth': 14, 'base_min_samples_leaf': 16}\n",
    "            # Use 'base_' prefix for base learner params:\n",
    "            base_max_depth = trial.suggest_int(\"base_max_depth\", 2, 20)\n",
    "            base_min_samples_leaf = trial.suggest_int(\"base_min_samples_leaf\", 2, 40)\n",
    "            base_learner = DecisionTreeRegressor(\n",
    "                criterion='friedman_mse', max_depth=base_max_depth, min_samples_leaf=base_min_samples_leaf\n",
    "                )\n",
    "            model = NGBRegressor(\n",
    "                verbose=False,\n",
    "                n_estimators=ngb_n_estimators, Base=base_learner\n",
    "                )\n",
    "            self.cv_score = cross_validate(model, X, y, cv=3)\n",
    "            return self.cv_score[\"test_score\"].mean()\n",
    "\n",
    "        self.study = optuna.create_study(direction=\"minimize\")\n",
    "        self.study.optimize(objective, n_trials=20)\n",
    "\n",
    "        \"\"\"Code below have to be modified to work with NGBoost\"\"\"\n",
    "        self.best_params = self.study.best_trials[0].params\n",
    "        base_learner = DecisionTreeRegressor(\n",
    "                criterion='friedman_mse',\n",
    "                max_depth=self.best_params[\"base_max_depth\"],\n",
    "                min_samples_leaf=self.best_params[\"base_min_samples_leaf\"]\n",
    "                )\n",
    "        self.model = NGBRegressor(\n",
    "                verbose=False,\n",
    "                n_estimators=self.best_params[\"ngb_n_estimators\"],\n",
    "                Base=base_learner\n",
    "                )\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        print(\"NGBoostRegressor trained.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def pred_dist(self, X):\n",
    "        \"\"\"\n",
    "        To access dictionary use 'params' property: ngbr.pred_dist(X).params\n",
    "        {'loc': array([15.71909047, 19.51384116, 19.24509285, 17.8645122 , 24.31325397]),\n",
    "        'scale': array([1.48748154, 1.37673424, 1.67090687, 1.63854999, 1.52513887])}\n",
    "        \"\"\"\n",
    "        return self.model.pred_dist(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1070c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGBoostRegressor initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/91 [01:30<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGBoostRegressor trained.\n",
      "step 0 mape 0.18141146423953516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ngb = NGBOOST()\n",
    "\n",
    "for i in tqdm(range(number_of_steps)):\n",
    "    \n",
    "    start = 51 + i*week_step\n",
    "    end = start + 52*years + i*week_step\n",
    "    \n",
    "    train_week_mask = (week_number >= start) & (week_number < end)\n",
    "    val_week_mask =  (week_number >= end) & (week_number < (end + week_step))\n",
    "\n",
    "    train_x, val_x = data.loc[train_week_mask], data.loc[val_week_mask]\n",
    "    train_y, val_y = target_sq.loc[train_week_mask], target_sq.loc[val_week_mask]\n",
    "\n",
    "    ngb.fit(train_x, train_y)\n",
    "\n",
    "    pred_proba = ngb.pred_dist(val_x)\n",
    "    \n",
    "    preds_df = pd.DataFrame(index=val_x.index, data=pred_proba.loc, columns=['ngb_pred'])\n",
    "    preds_df['ngb_std'] = pred_proba.scale\n",
    "    preds_df.to_csv(f'ngb_{i}.csv')\n",
    "    print(f\"step {i} mape {np.mean(np.abs(((pred_proba.loc - val_y) / val_y)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2879a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "631759b8335cb0b4b5aee9cb52c4cb4da20b0955a0b0c09d55e6406ad926d01f"
  },
  "kernelspec": {
   "display_name": "Python [conda env:prophet38]",
   "language": "python",
   "name": "conda-env-prophet38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
